{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import tensorflow as tf \n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import os  \n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import random \n",
    "from py_help import torch_helper as tc_help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tinygrad exists\r\n"
     ]
    }
   ],
   "source": [
    "## tiny grad setup\n",
    "! if [ ! -d tinygrad/.git ]; then git clone https://github.com/geohot/tinygrad.git ; cd tinygrad ; python3.8 setup.py develop ; else echo \"Tinygrad exists\"; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\r\n"
     ]
    }
   ],
   "source": [
    "# check gpu \n",
    "!python3.8 -c \"import torch; print(torch.cuda.is_available())\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Dataset/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt',\n '../Dataset/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt',\n '../Dataset/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = ['../Dataset/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/' + file for file in os.listdir('../Dataset/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/')]\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the dataset for label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24293578\\n',\n 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = tc_help().get_lines(files[0])\n",
    "train_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contents = tc_help().pre_processor(files[0])\n",
    "test_contents = tc_help().pre_processor(files[1])\n",
    "val_contents = tc_help().pre_processor(files[2])\n",
    "\n",
    "train_df = pd.DataFrame(train_contents)\n",
    "test_df = pd.DataFrame(test_contents)\n",
    "val_df = pd.DataFrame(val_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      target                                               text  line_number  \\\n0  OBJECTIVE  to investigate the efficacy of @ weeks of dail...            0   \n1    METHODS  a total of @ patients with primary knee oa wer...            1   \n2    METHODS  outcome measures included pain reduction and i...            2   \n3    METHODS  pain was assessed using the visual analog pain...            3   \n4    METHODS  secondary outcome measures included the wester...            4   \n\n   total_lines  \n0           11  \n1           11  \n2           11  \n3           11  \n4           11  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       target                                               text  line_number  \\\n0  BACKGROUND  this study analyzed liver function abnormaliti...            0   \n1     RESULTS  a post hoc analysis was conducted with the use...            1   \n2     RESULTS  liver function tests ( lfts ) were measured at...            2   \n3     RESULTS  survival analyses were used to assess the asso...            3   \n4     RESULTS  the percentage of patients with abnormal lfts ...            4   \n\n   total_lines  \n0            8  \n1            8  \n2            8  \n3            8  \n4            8  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       target                                               text  line_number  \\\n0  BACKGROUND  ige sensitization to aspergillus fumigatus and...            0   \n1  BACKGROUND  it is not clear whether these patients would b...            1   \n2   OBJECTIVE  we sought to determine whether a @-month cours...            2   \n3     METHODS  asthmatic patients who were ige sensitized to ...            3   \n4     METHODS  primary outcomes were improvement in quality o...            4   \n\n   total_lines  \n0            9  \n1            9  \n2            9  \n3            9  \n4            9  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO STOP WORD REMOVAL\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "swrds = stopwords.words(\"english\")\n",
    "print(swrds[:15])\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "def nltk_preprocessor(sentence,stopwords=swrds):\n",
    "        \"\"\"preprocessing the data based on nltk STOPWORDS\n",
    "\n",
    "        Args:\n",
    "            sentence (string): The string or the sentence that is to be passed \n",
    "\n",
    "        Returns:\n",
    "            sentence (string): The pre proceesed result from the function \n",
    "        \"\"\"\n",
    "\n",
    "        sentence = sentence.lower()\n",
    "        # get rid of the stop words\n",
    "        pt = re.compile(r\"\\b(\" + r\"|\".join(stopwords) + r\")\\b\\s*\")\n",
    "        sentence = pt.sub(\"\", sentence)\n",
    "        # paranthesis cases \n",
    "        sentence = re.sub(r\"\\([^)]*\\)\", \"\", sentence)\n",
    "        # handling the spaces and the filters\n",
    "        sentence = re.sub(r\"([-;;.,!?<=>])\", r\" \\1\", sentence)\n",
    "        sentence = re.sub(r\"[^A-Za-z0-9]\", \" \", sentence) # removing all cases for non alpha numeric characters \n",
    "        sentence = re.sub(\" +\", \" \", sentence)\n",
    "        sentence = sentence.strip()\n",
    "\n",
    "        return sentence \n",
    "\n",
    "prep_df = train_df.copy()\n",
    "prep_df.text = prep_df.text.apply(nltk_preprocessor)\n",
    "print(f\"{train_df.text.values[0]}\\n\\n{prep_df.text.values[0]}\")\n",
    "print(\"The number of sentences for training are : {} \\nThe number of sentences for vaildation are : {}\\n The number of sentences for testing are : {}\".format(len(train_df['text'].tolist()),len(val_df['text'].tolist()),len(test_df['text'].tolist())))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/markins/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours']\n",
      "to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .\n\ninvestigate efficacy weeks daily low dose oral prednisolone improving pain mobility systemic low grade inflammation short term whether effect would sustained weeks older adults moderate severe knee osteoarthritis\nThe number of sentences for training are : 180040 \nThe number of sentences for vaildation are : 30212\n The number of sentences for testing are : 30135\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "train_sz,val_sz,test_sz = 0.7,0.2,0.1\n",
    "x,y = prep_df['text'].values,prep_df['target'].values\n",
    "x_train,x_val,x_test,y_train,y_val,y_test = tc_help().data_splitter(x,y,train_sz)\n",
    "print('Trained Data shape ----> X_train : {} , Y_train : {} \\nValidation Data Shape -----> X_val : {} , Y_val : {}\\nTesting Data Shape -----> X_test : {} , Y_test : {}'.format(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Data shape ----> X_train : (126027,) , Y_train : (126027,) \nValidation Data Shape -----> X_val : (27006,) , Y_val : (27006,)\nTesting Data Shape -----> X_test : (27007,) , Y_test : (27007,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "lb = tc_help().lb_encoder\n",
    "lb.lb_fit(y_train)\n",
    "classes = lb.__length__()\n",
    "print('The nos label encoded classes : {}'.format(classes))\n",
    "lb.encoded_classes\n",
    "cl_names = lb.encoded_classes.keys()\n",
    "print(cl_names)\n",
    "# train_df['target'].values\n",
    "\n",
    "# targets to numbers\n",
    "y_train,y_val,y_test = lb.lb_encoder(train_df['target'].values),lb.lb_encoder(val_df['target'].values),lb.lb_encoder(test_df['target'].values)\n",
    "# weights of the classes\n",
    "cnts = np.bincount(y_train)\n",
    "clw = {index : 1.0/cnts for index , cnts in enumerate(cnts)}\n",
    "print(\"Counts and weights of the classes respectively : {} and \\n {} \".format(cnts,clw))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nos label encoded classes : 5\ndict_keys(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'])\nCounts and weights of the classes respectively : [21727 27168 59353 13839 57953] and \n {0: 4.6025682330740555e-05, 1: 3.680800942285041e-05, 2: 1.684834801947669e-05, 3: 7.225955632632416e-05, 4: 1.7255362103773747e-05} \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d9975a9822660044dfed5cbd2b2779158ae53db04b69fdf5bdd87965842cf3be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
